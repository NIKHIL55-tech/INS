{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GENERAL-PURPOSE NAS FOR TABULAR REGRESSION\n",
    "# Works for: AgroPlanner, INS Calibration, Sensor Modeling\n",
    "# ============================================================\n",
    "\n",
    "# -------------------- IMPORTS --------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# -------------------- DEVICE --------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ============================================================\n",
    "# 1. USER CONFIG (ONLY CHANGE THIS FOR NEW PROJECTS)\n",
    "# ============================================================\n",
    "\n",
    "CSV_PATH = \"dataset.csv\"          # Your dataset\n",
    "TARGET_COLUMN = \"yield\"           # e.g. \"yield\", \"bias_x\", \"scale_y\"\n",
    "CATEGORICAL_COLUMNS = [\"dist_name\", \"crop_type\"]  # [] for INS\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE = 0.15\n",
    "\n",
    "NAS_CONFIG = {\n",
    "    \"epochs\": 80,\n",
    "    \"batch_size\": 128,\n",
    "    \"trials\": 30,\n",
    "    \"patience\": 8\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# 2. DATA LOADING & PREPROCESSING\n",
    "# ============================================================\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Encode categorical features\n",
    "for col in CATEGORICAL_COLUMNS:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "X = df.drop(columns=[TARGET_COLUMN]).values\n",
    "y = df[TARGET_COLUMN].values.reshape(-1, 1)\n",
    "\n",
    "# Standardization (CRITICAL for INS)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split: Train / Val / Test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=(TEST_SIZE + VAL_SIZE), random_state=42\n",
    ")\n",
    "\n",
    "relative_val_size = VAL_SIZE / (TEST_SIZE + VAL_SIZE)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=relative_val_size, random_state=42\n",
    ")\n",
    "\n",
    "def to_tensor(x, y):\n",
    "    return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "X_train, y_train = to_tensor(X_train, y_train)\n",
    "X_val, y_val = to_tensor(X_val, y_val)\n",
    "X_test, y_test = to_tensor(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(X_train, y_train),\n",
    "    batch_size=NAS_CONFIG[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    TensorDataset(X_val, y_val),\n",
    "    batch_size=NAS_CONFIG[\"batch_size\"],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(X_test, y_test),\n",
    "    batch_size=NAS_CONFIG[\"batch_size\"],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "INPUT_DIM = X_train.shape[1]\n",
    "\n",
    "# ============================================================\n",
    "# 3. NAS MODEL\n",
    "# ============================================================\n",
    "\n",
    "class NASNet(nn.Module):\n",
    "    def __init__(self, input_dim, layers, activation, dropout):\n",
    "        super().__init__()\n",
    "        net = []\n",
    "        prev = input_dim\n",
    "\n",
    "        for h in layers:\n",
    "            net += [\n",
    "                nn.Linear(prev, h),\n",
    "                nn.BatchNorm1d(h),\n",
    "                activation(),\n",
    "                nn.Dropout(dropout)\n",
    "            ]\n",
    "            prev = h\n",
    "\n",
    "        net.append(nn.Linear(prev, 1))\n",
    "        self.model = nn.Sequential(*net)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ============================================================\n",
    "# 4. METRICS\n",
    "# ============================================================\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mse, rmse, mae, r2\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            preds = model(x.to(DEVICE)).cpu().numpy()\n",
    "            y_true.extend(y.numpy())\n",
    "            y_pred.extend(preds)\n",
    "\n",
    "    return compute_metrics(np.array(y_true), np.array(y_pred))\n",
    "\n",
    "# ============================================================\n",
    "# 5. TRAIN LOOP\n",
    "# ============================================================\n",
    "\n",
    "def train_epoch(model, optimizer, criterion):\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(x.to(DEVICE)), y.to(DEVICE))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# ============================================================\n",
    "# 6. NAS OBJECTIVE\n",
    "# ============================================================\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 4)\n",
    "    layers = [\n",
    "        trial.suggest_int(f\"units_l{i}\", 32, 192, step=32)\n",
    "        for i in range(n_layers)\n",
    "    ]\n",
    "\n",
    "    activation_name = trial.suggest_categorical(\n",
    "        \"activation\", [\"ReLU\", \"LeakyReLU\", \"ELU\"]\n",
    "    )\n",
    "    activation = {\n",
    "        \"ReLU\": nn.ReLU,\n",
    "        \"LeakyReLU\": nn.LeakyReLU,\n",
    "        \"ELU\": nn.ELU\n",
    "    }[activation_name]\n",
    "\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    wd = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "\n",
    "    model = NASNet(INPUT_DIM, layers, activation, dropout).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    patience = 0\n",
    "\n",
    "    for epoch in range(NAS_CONFIG[\"epochs\"]):\n",
    "        train_epoch(model, optimizer, criterion)\n",
    "        val_mse, _, _, _ = evaluate(model, val_loader)\n",
    "\n",
    "        trial.report(val_mse, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if val_mse < best_val:\n",
    "            best_val = val_mse\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "        if patience >= NAS_CONFIG[\"patience\"]:\n",
    "            break\n",
    "\n",
    "    val_mse, val_rmse, val_mae, val_r2 = evaluate(model, val_loader)\n",
    "\n",
    "    print(\n",
    "        f\"Trial {trial.number} | \"\n",
    "        f\"MSE: {val_mse:.4f}, RMSE: {val_rmse:.4f}, \"\n",
    "        f\"MAE: {val_mae:.4f}, R2: {val_r2:.4f}\"\n",
    "    )\n",
    "\n",
    "    trial.set_user_attr(\"MSE\", val_mse)\n",
    "    trial.set_user_attr(\"RMSE\", val_rmse)\n",
    "    trial.set_user_attr(\"MAE\", val_mae)\n",
    "    trial.set_user_attr(\"R2\", val_r2)\n",
    "\n",
    "    return val_mse\n",
    "\n",
    "# ============================================================\n",
    "# 7. RUN NAS\n",
    "# ============================================================\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    ")\n",
    "study.optimize(objective, n_trials=NAS_CONFIG[\"trials\"])\n",
    "\n",
    "# ============================================================\n",
    "# 8. BEST MODEL EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "params = study.best_params\n",
    "\n",
    "best_layers = [params[f\"units_l{i}\"] for i in range(params[\"n_layers\"])]\n",
    "best_activation = {\n",
    "    \"ReLU\": nn.ReLU,\n",
    "    \"LeakyReLU\": nn.LeakyReLU,\n",
    "    \"ELU\": nn.ELU\n",
    "}[params[\"activation\"]]\n",
    "\n",
    "final_model = NASNet(\n",
    "    INPUT_DIM, best_layers, best_activation, params[\"dropout\"]\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    final_model.parameters(),\n",
    "    lr=params[\"lr\"],\n",
    "    weight_decay=params[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for _ in range(NAS_CONFIG[\"epochs\"]):\n",
    "    train_epoch(final_model, optimizer, criterion)\n",
    "\n",
    "print(\"\\n====== FINAL MODEL METRICS ======\")\n",
    "print(\"TRAIN:\", evaluate(final_model, train_loader))\n",
    "print(\"VAL  :\", evaluate(final_model, val_loader))\n",
    "print(\"TEST :\", evaluate(final_model, test_loader))\n",
    "\n",
    "torch.save(final_model.state_dict(), \"best_nas_model.pth\")\n",
    "print(\"\\n✔ Saved best_nas_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb3ae7b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# AgroPlanner – Neural Architecture Search with Full Metrics\n",
    "# ============================================================\n",
    "\n",
    "# -------------------- IMPORTS --------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# -------------------- GPU CONFIG --------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# -------------------- NAS CONFIG --------------------\n",
    "EPOCHS = 80\n",
    "BATCH_SIZE = 128\n",
    "N_TRIALS = 30\n",
    "PATIENCE = 8\n",
    "\n",
    "# -------------------- LOAD DATA --------------------\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "TARGET = \"yield\"\n",
    "CATEGORICAL_COLS = [\"dist_name\", \"crop_type\"]\n",
    "NUMERIC_COLS = [c for c in df.columns if c not in CATEGORICAL_COLS + [TARGET]]\n",
    "\n",
    "# Encode categorical columns\n",
    "for col in CATEGORICAL_COLS:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "X = df[CATEGORICAL_COLS + NUMERIC_COLS].values\n",
    "y = df[TARGET].values.reshape(-1, 1)\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# -------------------- SPLITS --------------------\n",
    "# 70% Train, 15% Val, 15% Test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# -------------------- TENSORS --------------------\n",
    "def to_tensor(x, y):\n",
    "    return (\n",
    "        torch.tensor(x, dtype=torch.float32),\n",
    "        torch.tensor(y, dtype=torch.float32)\n",
    "    )\n",
    "\n",
    "X_train, y_train = to_tensor(X_train, y_train)\n",
    "X_val, y_val     = to_tensor(X_val, y_val)\n",
    "X_test, y_test   = to_tensor(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(X_train, y_train),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    TensorDataset(X_val, y_val),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(X_test, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "INPUT_DIM = X_train.shape[1]\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "# -------------------- MODEL --------------------\n",
    "class NASNet(nn.Module):\n",
    "    def __init__(self, input_dim, layers, activation, dropout):\n",
    "        super().__init__()\n",
    "        modules = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        for h in layers:\n",
    "            modules.append(nn.Linear(prev_dim, h))\n",
    "            modules.append(nn.BatchNorm1d(h))\n",
    "            modules.append(activation())\n",
    "            modules.append(nn.Dropout(dropout))\n",
    "            prev_dim = h\n",
    "\n",
    "        modules.append(nn.Linear(prev_dim, OUTPUT_DIM))\n",
    "        self.net = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# -------------------- METRICS --------------------\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mse, rmse, mae, r2\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(DEVICE)\n",
    "            preds = model(x).cpu().numpy()\n",
    "            y_true.extend(y.numpy())\n",
    "            y_pred.extend(preds)\n",
    "\n",
    "    return compute_metrics(np.array(y_true), np.array(y_pred))\n",
    "\n",
    "# -------------------- TRAIN LOOP --------------------\n",
    "def train_epoch(model, optimizer, criterion):\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(DEVICE, non_blocking=True)\n",
    "        y = y.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(x), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# -------------------- OPTUNA OBJECTIVE --------------------\n",
    "def objective(trial):\n",
    "\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 4)\n",
    "    layers = [\n",
    "        trial.suggest_int(f\"units_l{i}\", 32, 192, step=32)\n",
    "        for i in range(n_layers)\n",
    "    ]\n",
    "\n",
    "    activation_name = trial.suggest_categorical(\n",
    "        \"activation\", [\"ReLU\", \"LeakyReLU\", \"ELU\"]\n",
    "    )\n",
    "    activation = {\n",
    "        \"ReLU\": nn.ReLU,\n",
    "        \"LeakyReLU\": nn.LeakyReLU,\n",
    "        \"ELU\": nn.ELU\n",
    "    }[activation_name]\n",
    "\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "\n",
    "    model = NASNet(INPUT_DIM, layers, activation, dropout).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    patience = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_epoch(model, optimizer, criterion)\n",
    "        val_mse, _, _, _ = evaluate_model(model, val_loader)\n",
    "\n",
    "        trial.report(val_mse, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if val_mse < best_val:\n",
    "            best_val = val_mse\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "        if patience >= PATIENCE:\n",
    "            break\n",
    "\n",
    "    val_mse, val_rmse, val_mae, val_r2 = evaluate_model(model, val_loader)\n",
    "\n",
    "    print(\n",
    "        f\"Trial {trial.number} | \"\n",
    "        f\"MSE: {val_mse:.4f} | RMSE: {val_rmse:.4f} | \"\n",
    "        f\"MAE: {val_mae:.4f} | R2: {val_r2:.4f}\"\n",
    "    )\n",
    "\n",
    "    trial.set_user_attr(\"MSE\", val_mse)\n",
    "    trial.set_user_attr(\"RMSE\", val_rmse)\n",
    "    trial.set_user_attr(\"MAE\", val_mae)\n",
    "    trial.set_user_attr(\"R2\", val_r2)\n",
    "\n",
    "    return val_mse\n",
    "\n",
    "# -------------------- RUN NAS --------------------\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    ")\n",
    "study.optimize(objective, n_trials=N_TRIALS)\n",
    "\n",
    "# -------------------- PRINT ALL RESULTS --------------------\n",
    "print(\"\\n========= ALL NAS TRIAL RESULTS =========\")\n",
    "for t in study.trials:\n",
    "    if t.state.name == \"COMPLETE\":\n",
    "        print(\n",
    "            f\"Trial {t.number} | \"\n",
    "            f\"MSE: {t.user_attrs['MSE']:.4f}, \"\n",
    "            f\"RMSE: {t.user_attrs['RMSE']:.4f}, \"\n",
    "            f\"MAE: {t.user_attrs['MAE']:.4f}, \"\n",
    "            f\"R2: {t.user_attrs['R2']:.4f}\"\n",
    "        )\n",
    "\n",
    "# -------------------- BEST MODEL --------------------\n",
    "params = study.best_params\n",
    "\n",
    "best_layers = [params[f\"units_l{i}\"] for i in range(params[\"n_layers\"])]\n",
    "best_activation = {\n",
    "    \"ReLU\": nn.ReLU,\n",
    "    \"LeakyReLU\": nn.LeakyReLU,\n",
    "    \"ELU\": nn.ELU\n",
    "}[params[\"activation\"]]\n",
    "\n",
    "final_model = NASNet(\n",
    "    INPUT_DIM, best_layers, best_activation, params[\"dropout\"]\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    final_model.parameters(),\n",
    "    lr=params[\"lr\"],\n",
    "    weight_decay=params[\"weight_decay\"]\n",
    ")\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for _ in range(EPOCHS):\n",
    "    train_epoch(final_model, optimizer, criterion)\n",
    "\n",
    "# -------------------- FINAL METRICS --------------------\n",
    "train_metrics = evaluate_model(final_model, train_loader)\n",
    "val_metrics   = evaluate_model(final_model, val_loader)\n",
    "test_metrics  = evaluate_model(final_model, test_loader)\n",
    "\n",
    "print(\"\\n========= BEST MODEL METRICS =========\")\n",
    "\n",
    "print(\"TRAIN:\")\n",
    "print(f\"MSE: {train_metrics[0]:.4f}, RMSE: {train_metrics[1]:.4f}, \"\n",
    "      f\"MAE: {train_metrics[2]:.4f}, R2: {train_metrics[3]:.4f}\")\n",
    "\n",
    "print(\"\\nVALIDATION:\")\n",
    "print(f\"MSE: {val_metrics[0]:.4f}, RMSE: {val_metrics[1]:.4f}, \"\n",
    "      f\"MAE: {val_metrics[2]:.4f}, R2: {val_metrics[3]:.4f}\")\n",
    "\n",
    "print(\"\\nTEST:\")\n",
    "print(f\"MSE: {test_metrics[0]:.4f}, RMSE: {test_metrics[1]:.4f}, \"\n",
    "      f\"MAE: {test_metrics[2]:.4f}, R2: {test_metrics[3]:.4f}\")\n",
    "\n",
    "# -------------------- SAVE MODEL --------------------\n",
    "torch.save(final_model.state_dict(), \"best_nas_model.pth\")\n",
    "print(\"\\n✔ Best NAS model saved as best_nas_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1c9b05",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "model.train()\n",
    "\n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=True\n",
    ") as prof:\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        if batch_idx >= 5:   # profile first 5 batches only\n",
    "            break\n",
    "\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        with record_function(\"forward\"):\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "\n",
    "        with record_function(\"backward\"):\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "print(\n",
    "    prof.key_averages()\n",
    "    .table(sort_by=\"cuda_time_total\", row_limit=15)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
